{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example api"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from adelecv.api.logs import enable_logs\n",
    "from adelecv.api.data.segmentations import SegmentationDataset\n",
    "from logging import StreamHandler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To enable logs, there is a function\n",
    " ```python\n",
    "enable logs\n",
    " ```\n",
    "You can pass any necessary logging hundler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "enable_logs(StreamHandler())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from adelecv.api.config import Settings\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# you can change the path to save logs and weights\n",
    "# Settings.update_tmp_path(Path('your_path'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from adelecv.api.config import Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "WindowsPath('tmp/9fa1cc64111e44bb9dfe9650797dcf25')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.TMP_PATH"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. You need to create a dataset and pass the necessary parameters to it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from adelecv.api.data.segmentations.types import ImageMask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 200/200 [1.3s elapsed, 0s remaining, 152.9 samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Split dataset train size: 140, valid size: 40, test size: 20\n",
      "INFO - Creating a dataset\n",
      "DEBUG - Dataset created with params, dataset dir: F:\\dataset\\ph2, classes: {0: 'background', 1: 'target'}, batch size: 16\n"
     ]
    }
   ],
   "source": [
    "segm_dataset = SegmentationDataset(\n",
    "    r'F:\\dataset\\ph2',  # path to dataset\n",
    "    ImageMask,          # dataset type\n",
    "    (256, 256),         # img size HxW\n",
    "    (0.7, 0.2, 0.1),    # train val test split\n",
    "    16,                 # batch size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get possible values for hyperparameters, there are functions presented below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from adelecv.api.models.segmentations import  get_encoders, get_pretrained_weights, get_models, get_torch_optimizers, get_losses, get_optimize_scores\n",
    "from adelecv.api.optimize.segmentations import get_hp_optimizers\n",
    "# создать отдельный модуль для получения возможных параметров"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models:  ['Unet', 'UnetPlusPlus', 'MAnet', 'Linknet', 'FPN', 'PSPNet', 'DeepLabV3', 'DeepLabV3Plus', 'PAN']\n",
      "encoders:  ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x4d', 'resnext101_32x8d', 'resnext101_32x16d', 'resnext101_32x32d', 'resnext101_32x48d', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'inceptionresnetv2', 'inceptionv4', 'efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5', 'efficientnet-b6', 'efficientnet-b7', 'mobilenet_v2', 'xception', 'timm-efficientnet-b0', 'timm-efficientnet-b1', 'timm-efficientnet-b2', 'timm-efficientnet-b3', 'timm-efficientnet-b4', 'timm-efficientnet-b5', 'timm-efficientnet-b6', 'timm-efficientnet-b7', 'timm-efficientnet-b8', 'timm-efficientnet-l2', 'timm-tf_efficientnet_lite0', 'timm-tf_efficientnet_lite1', 'timm-tf_efficientnet_lite2', 'timm-tf_efficientnet_lite3', 'timm-tf_efficientnet_lite4', 'timm-resnest14d', 'timm-resnest26d', 'timm-resnest50d', 'timm-resnest101e', 'timm-resnest200e', 'timm-resnest269e', 'timm-resnest50d_4s2x40d', 'timm-resnest50d_1s4x24d', 'timm-res2net50_26w_4s', 'timm-res2net101_26w_4s', 'timm-res2net50_26w_6s', 'timm-res2net50_26w_8s', 'timm-res2net50_48w_2s', 'timm-res2net50_14w_8s', 'timm-res2next50', 'timm-regnetx_002', 'timm-regnetx_004', 'timm-regnetx_006', 'timm-regnetx_008', 'timm-regnetx_016', 'timm-regnetx_032', 'timm-regnetx_040', 'timm-regnetx_064', 'timm-regnetx_080', 'timm-regnetx_120', 'timm-regnetx_160', 'timm-regnetx_320', 'timm-regnety_002', 'timm-regnety_004', 'timm-regnety_006', 'timm-regnety_008', 'timm-regnety_016', 'timm-regnety_032', 'timm-regnety_040', 'timm-regnety_064', 'timm-regnety_080', 'timm-regnety_120', 'timm-regnety_160', 'timm-regnety_320', 'timm-skresnet18', 'timm-skresnet34', 'timm-skresnext50_32x4d', 'timm-mobilenetv3_large_075', 'timm-mobilenetv3_large_100', 'timm-mobilenetv3_large_minimal_100', 'timm-mobilenetv3_small_075', 'timm-mobilenetv3_small_100', 'timm-mobilenetv3_small_minimal_100', 'timm-gernet_s', 'timm-gernet_m', 'timm-gernet_l', 'mit_b0', 'mit_b1', 'mit_b2', 'mit_b3', 'mit_b4', 'mit_b5', 'mobileone_s0', 'mobileone_s1', 'mobileone_s2', 'mobileone_s3', 'mobileone_s4']\n",
      "pretrained_weights ['imagenet', 'None']\n",
      "optimizers ['AdamW', 'Adadelta', 'Adam', 'SGD', 'RAdam', 'NAdam', 'RMSprop', 'Adagrad']\n",
      "losses ['JaccardLoss', 'DiceLoss', 'FocalLoss', 'LovaszLoss', 'SoftBCEWithLogitsLoss', 'SoftCrossEntropyLoss', 'TverskyLoss', 'MCCLoss']\n",
      "hp optimizers ['RandomSampler', 'GridSampler', 'TPESampler', 'CmaEsSampler', 'NSGAIISampler', 'QMCSampler', 'MOTPESampler']\n",
      "optimize scores ['fbeta_score', 'f1_score', 'iou_score', 'accuracy', 'positive_predictive_value', 'sensitivity', 'loss']\n"
     ]
    }
   ],
   "source": [
    "print('models: ', get_models())\n",
    "print('encoders: ', get_encoders())\n",
    "print('pretrained_weights', get_pretrained_weights())\n",
    "print('optimizers', get_torch_optimizers())\n",
    "print('losses', get_losses())\n",
    "print('hp optimizers', get_hp_optimizers())\n",
    "print('optimize scores', get_optimize_scores())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. It is necessary to create a set of hyperparameters, according to which the optimal ones will be searched."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from adelecv.api.optimize.segmentations import HyperParamsSegmentation\n",
    "from adelecv.api.optimize.segmentations import HPOptimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "params = HyperParamsSegmentation(\n",
    "    strategy='TPESampler',\n",
    "    architectures=['UnetPlusPlus'],\n",
    "    encoders=['mobilenet_v2'],\n",
    "    pretrained_weights=['imagenet'],\n",
    "    loss_fns=['JaccardLoss'],\n",
    "    optimizers=['AdamW', 'RMSprop'],\n",
    "    epoch_range=(5, 5),\n",
    "    lr_range=(0.001, 0.003),\n",
    "    optimize_score='loss',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. You need to create an optimizer object with the transfer of the dataset and hyperparameters to it. You also need to choose an optimization algorithm and set additional training parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Create hp optimizer with params: strategy: TPESampler num_trials: 1 num_classes: 2 device: cuda hps: {'strategy': 'TPESampler', 'architectures': ['UnetPlusPlus'], 'encoders': ['mobilenet_v2'], 'pretrained_weights': ['imagenet'], 'loss_fns': ['JaccardLoss'], 'optimizers': ['AdamW', 'RMSprop'], 'epoch_range': (5, 5), 'lr_range': (0.001, 0.003), 'optimize_score': 'loss'}\n",
      "DEBUG - Create hp optimizer with params: strategy: TPESampler num_trials: 3 num_classes: 2 device: cuda hps: {'strategy': 'TPESampler', 'architectures': ['UnetPlusPlus'], 'encoders': ['mobilenet_v2'], 'pretrained_weights': ['imagenet'], 'loss_fns': ['JaccardLoss'], 'optimizers': ['AdamW', 'RMSprop'], 'epoch_range': (5, 5), 'lr_range': (0.001, 0.003), 'optimize_score': 'loss'}\n"
     ]
    }
   ],
   "source": [
    "hp_optimizer = HPOptimizer(\n",
    "    hyper_params=params,\n",
    "    num_trials=3,\n",
    "    device='GPU',\n",
    "    dataset=segm_dataset\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Tensorboard is used to visualize learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from adelecv.api.config import Settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "tensorboard_logs_path = Settings.TENSORBOARD_LOGS_PATH.as_posix()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'tmp/cb0cb0afd3f84fb6b0d8299532ad8759/tensorboard'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_logs_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'tmp/cb0cb0afd3f84fb6b0d8299532ad8759/tensorboard'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_logs_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-10919857392befa1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-10919857392befa1\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir $tensorboard_logs_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. To start the training, call the appropriate method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Train models started\n",
      "\u001B[32m[I 2023-05-01 17:46:09,162]\u001B[0m A new study created in memory with name: no-name-f6be76f6-855f-4d0f-960b-047ecab722b2\u001B[0m\n",
      "DEBUG - Create tensorboard logger, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/tensorboard/499ba36\n",
      "DEBUG - Dataset updated\n",
      "INFO - Model 499ba36_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0022375857163229526 trained with test loss 0.14925605058670044\n",
      "DEBUG - Add predictions for model: 499ba36_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0022375857163229526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 [9.4s elapsed, 20.8 samples/s]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Save weights model: 499ba36_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0022375857163229526, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/weights/499ba36.pt\n",
      "\u001B[32m[I 2023-05-01 17:47:01,928]\u001B[0m Trial 0 finished with value: 0.11668000618616739 and parameters: {'optimizer': 'AdamW', 'architecture': 'UnetPlusPlus', 'encoders': 'mobilenet_v2', 'pretrained_weight': 'imagenet', 'lr': 0.0022375857163229526, 'loss': 'JaccardLoss', 'num_epoch': 5}. Best is trial 0 with value: 0.11668000618616739.\u001B[0m\n",
      "DEBUG - Create tensorboard logger, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/tensorboard/d81e347\n",
      "DEBUG - Dataset updated\n",
      "INFO - Model d81e347_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0020570884365662253 trained with test loss 0.13842931389808655\n",
      "DEBUG - Add predictions for model: d81e347_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0020570884365662253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 [12.1s elapsed, 15.4 samples/s]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Save weights model: d81e347_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0020570884365662253, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/weights/d81e347.pt\n",
      "\u001B[32m[I 2023-05-01 17:47:54,924]\u001B[0m Trial 1 finished with value: 0.10880672931671143 and parameters: {'optimizer': 'AdamW', 'architecture': 'UnetPlusPlus', 'encoders': 'mobilenet_v2', 'pretrained_weight': 'imagenet', 'lr': 0.0020570884365662253, 'loss': 'JaccardLoss', 'num_epoch': 5}. Best is trial 1 with value: 0.10880672931671143.\u001B[0m\n",
      "DEBUG - Create tensorboard logger, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/tensorboard/304731e\n",
      "DEBUG - Dataset updated\n",
      "INFO - Model 304731e_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0025402917390305577 trained with test loss 0.09902772307395935\n",
      "DEBUG - Add predictions for model: 304731e_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0025402917390305577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 200 [15.2s elapsed, 11.6 samples/s]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Save weights model: 304731e_UnetPlusPlus_mobilenet_v2_imagenet_AdamW_JaccardLoss_lr=0,0025402917390305577, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/weights/304731e.pt\n",
      "\u001B[32m[I 2023-05-01 17:48:53,994]\u001B[0m Trial 2 finished with value: 0.08640851577123006 and parameters: {'optimizer': 'AdamW', 'architecture': 'UnetPlusPlus', 'encoders': 'mobilenet_v2', 'pretrained_weight': 'imagenet', 'lr': 0.0025402917390305577, 'loss': 'JaccardLoss', 'num_epoch': 5}. Best is trial 2 with value: 0.08640851577123006.\u001B[0m\n",
      "INFO - Study statistics:\n",
      "INFO - Number of finished trials: 3\n",
      "INFO - Number of pruned trials: 0\n",
      "INFO - Number of complete trials: 3\n",
      "INFO - Best trial:\n",
      "INFO - Value: 0.08640851577123006\n",
      "INFO - Params: \n",
      "INFO - optimizer: AdamW\n",
      "INFO - architecture: UnetPlusPlus\n",
      "INFO - encoders: mobilenet_v2\n",
      "INFO - pretrained_weight: imagenet\n",
      "INFO - lr: 0.0025402917390305577\n",
      "INFO - loss: JaccardLoss\n",
      "INFO - num_epoch: 5\n",
      "INFO - Train models is over\n"
     ]
    }
   ],
   "source": [
    "hp_optimizer.optimize()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "       _id                                               name  architecture  \\\n0  499ba36  499ba36_UnetPlusPlus_mobilenet_v2_imagenet_Ada...  UnetPlusPlus   \n1  d81e347  d81e347_UnetPlusPlus_mobilenet_v2_imagenet_Ada...  UnetPlusPlus   \n2  304731e  304731e_UnetPlusPlus_mobilenet_v2_imagenet_Ada...  UnetPlusPlus   \n\n        encoder pretrained_weight        lr optimizer      loss_fn  num_epoch  \\\n0  mobilenet_v2          imagenet  0.002238     AdamW  JaccardLoss          5   \n1  mobilenet_v2          imagenet  0.002057     AdamW  JaccardLoss          5   \n2  mobilenet_v2          imagenet  0.002540     AdamW  JaccardLoss          5   \n\n    loss  fbeta_score  f1_score  iou_score  accuracy  \\\n0  0.149        0.868     0.868      0.810     0.931   \n1  0.138        0.876     0.876      0.823     0.934   \n2  0.099        0.928     0.928      0.884     0.952   \n\n   positive_predictive_value  sensitivity  \n0                      0.900        0.901  \n1                      0.890        0.929  \n2                      0.935        0.944  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>name</th>\n      <th>architecture</th>\n      <th>encoder</th>\n      <th>pretrained_weight</th>\n      <th>lr</th>\n      <th>optimizer</th>\n      <th>loss_fn</th>\n      <th>num_epoch</th>\n      <th>loss</th>\n      <th>fbeta_score</th>\n      <th>f1_score</th>\n      <th>iou_score</th>\n      <th>accuracy</th>\n      <th>positive_predictive_value</th>\n      <th>sensitivity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>499ba36</td>\n      <td>499ba36_UnetPlusPlus_mobilenet_v2_imagenet_Ada...</td>\n      <td>UnetPlusPlus</td>\n      <td>mobilenet_v2</td>\n      <td>imagenet</td>\n      <td>0.002238</td>\n      <td>AdamW</td>\n      <td>JaccardLoss</td>\n      <td>5</td>\n      <td>0.149</td>\n      <td>0.868</td>\n      <td>0.868</td>\n      <td>0.810</td>\n      <td>0.931</td>\n      <td>0.900</td>\n      <td>0.901</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d81e347</td>\n      <td>d81e347_UnetPlusPlus_mobilenet_v2_imagenet_Ada...</td>\n      <td>UnetPlusPlus</td>\n      <td>mobilenet_v2</td>\n      <td>imagenet</td>\n      <td>0.002057</td>\n      <td>AdamW</td>\n      <td>JaccardLoss</td>\n      <td>5</td>\n      <td>0.138</td>\n      <td>0.876</td>\n      <td>0.876</td>\n      <td>0.823</td>\n      <td>0.934</td>\n      <td>0.890</td>\n      <td>0.929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>304731e</td>\n      <td>304731e_UnetPlusPlus_mobilenet_v2_imagenet_Ada...</td>\n      <td>UnetPlusPlus</td>\n      <td>mobilenet_v2</td>\n      <td>imagenet</td>\n      <td>0.002540</td>\n      <td>AdamW</td>\n      <td>JaccardLoss</td>\n      <td>5</td>\n      <td>0.099</td>\n      <td>0.928</td>\n      <td>0.928</td>\n      <td>0.884</td>\n      <td>0.952</td>\n      <td>0.935</td>\n      <td>0.944</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_optimizer.stats_models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "6 (Optional). A special class is provided for exporting weights (creating a zip archive). Mainly used for the web version of the library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "from adelecv.api.modification_models import ExportWeights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# you can save the selected weights in a zip file (feature for ui)\n",
    "# ExportWeights().create_zip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. You can visualize the datasets and prediction of each model using the fiftyone."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.lib.display.IFrame at 0x255cc6855d0>",
      "text/html": "\n        <iframe\n            width=\"100%\"\n            height=\"800\"\n            src=\"http://localhost:5151/?notebook=True&subscription=5a05bc1c-e14b-4099-8220-2ca2dff9392c\"\n            frameborder=\"0\"\n            allowfullscreen\n            \n        ></iframe>\n        "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset:          ImageMask\nMedia type:       image\nNum samples:      200\nSelected samples: 0\nSelected labels:  0\nSession URL:      http://localhost:5151/"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fo.launch_app(segm_dataset.fo_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. To convert weights, you can use the appropriate classes. Below is an example for the onnx format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from adelecv.api.modification_models import ConvertWeights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "cw = ConvertWeights(input_shape=(1, 3, 256, 256))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Сonvert weights model: 304731e to onnx format\n",
      "D:\\anaconda\\envs\\adelecv-check\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:16: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if h % output_stride != 0 or w % output_stride != 0:\n",
      "D:\\anaconda\\envs\\adelecv-check\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:829: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Create zip with converted weights, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/converted_bdb48d4c20604da9912eb70d68588744.zip\n",
      "INFO - Сonvert weights model: 499ba36 to onnx format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Create zip with converted weights, path: tmp/9fa1cc64111e44bb9dfe9650797dcf25/converted_bdb48d4c20604da9912eb70d68588744.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": "WindowsPath('tmp/9fa1cc64111e44bb9dfe9650797dcf25/converted_bdb48d4c20604da9912eb70d68588744.zip')"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw.run(id_selected=['304731e', '499ba36'], new_format='onnx')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
